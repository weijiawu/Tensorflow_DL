{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp\n",
    "RMSprop 是由 Geoff Hinton 在他 Coursera 课程中提出的一种适应性学习率方法，至今仍未被公开发表。前面我们提到了 Adagrad 算法有一个问题，就是学习率分母上的变量 s 不断被累加增大，最后会导致学习率除以一个比较大的数之后变得非常小，这不利于我们找到最后的最优解，所以 RMSProp 的提出就是为了解决这个问题。\n",
    "\n",
    "## RMSProp 算法\n",
    "RMSProp 仍然会使用梯度的平方量，不同于 Adagrad，其会使用一个指数加权移动平均来计算这个 s，也就是\n",
    "\n",
    "$$\n",
    "s_i = \\alpha s_{i-1} + (1 - \\alpha) \\ g^2\n",
    "$$\n",
    "\n",
    "这里 g 表示当前求出的参数梯度，然后最终更新和 Adagrad 是一样的，学习率变成了\n",
    "\n",
    "$$\n",
    "\\frac{\\eta}{\\sqrt{s + \\epsilon}}\n",
    "$$\n",
    "\n",
    "这里 $\\alpha$ 是一个移动平均的系数，也是因为这个系数，导致了 RMSProp 和 Adagrad 不同的地方，这个系数使得 RMSProp 更新到后期累加的梯度平方较小，从而保证 s 不会太大，也就使得模型后期依然能够找到比较优的结果\n",
    "\n",
    "实现上和 Adagrad 非常像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.layers import hidden_layer, DNN\n",
    "\n",
    "tf.set_random_seed(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据导入\n",
    "mnist = input_data.read_data_sets('../MNIST_data', one_hot=True)\n",
    "\n",
    "train_set = mnist.train\n",
    "test_set = mnist.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "input_ph = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n",
    "label_ph = tf.placeholder(shape=(None, 10), dtype=tf.int64)\n",
    "\n",
    "dnn = DNN(input_ph, [200], weights_collection='params', biases_collection='params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建`loss`和`acc`\n",
    "loss = tf.losses.softmax_cross_entropy(logits=dnn, onehot_labels=label_ph)\n",
    "\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(dnn, axis=-1), tf.argmax(label_ph, axis=-1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取梯度\n",
    "params = tf.get_collection('params')\n",
    "\n",
    "gradients = tf.gradients(loss, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义`RMSProp`更新算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop_update(params, gradients, sqrs, lr, alpha, name='adagrad_update'):\n",
    "    eps = 1e-10\n",
    "    \n",
    "    update_ops = []\n",
    "    for param, gradient, sqr in zip(params, gradients, sqrs):\n",
    "        sqr_update = sqr.assign(alpha * sqr + (1 - alpha) * tf.square(gradient))\n",
    "        with tf.control_dependencies([sqr_update]):\n",
    "            delta = lr / tf.sqrt(sqr + eps) * gradient\n",
    "            update_ops.append(param.assign_sub(delta))\n",
    "            \n",
    "    update_op = tf.group(*update_ops, name=name)\n",
    "    return update_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义辅助变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sqrs'):\n",
    "    for i, param in enumerate(params):\n",
    "        v = tf.get_variable(param.op.name, shape=param.get_shape(), initializer=tf.zeros_initializer(), dtype=tf.float32)\n",
    "        tf.add_to_collection('sqrs', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrs = tf.get_collection('sqrs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用rmsprop定义更新`op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_op = rmsprop_update(params, gradients, sqrs, 0.01, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "epoch = 0\n",
    "samples_passed = 0\n",
    "epoch_done = False\n",
    "step = 0\n",
    "\n",
    "_start = time.time()\n",
    "while (epoch < 5):\n",
    "    if samples_passed + batch_size >= mnist.train.num_examples:\n",
    "        this_batch = mnist.train.num_examples - samples_passed\n",
    "        samples_passed = 0\n",
    "        epoch += 1\n",
    "        epoch_done = True\n",
    "    else:\n",
    "        samples_passed += batch_size\n",
    "        this_batch = batch_size\n",
    "        \n",
    "    # 获取 batch_size个训练样本\n",
    "    images, labels = train_set.next_batch(this_batch)\n",
    "    if epoch_done:\n",
    "        # 计算所有训练样本的损失值\n",
    "        train_loss = []\n",
    "        for _ in range(train_set.num_examples // 100):\n",
    "            image, label = train_set.next_batch(100)\n",
    "            loss_train = sess.run(loss, feed_dict={input_ph: image, label_ph: label})\n",
    "            train_loss.append(loss_train)\n",
    "\n",
    "        print('Epoch {} Train loss: {:.6f}'.format(epoch, np.array(train_loss).mean()))\n",
    "        epoch_done = False\n",
    "        \n",
    "    # 每30步记录一次训练误差\n",
    "    if step % 30 == 0:\n",
    "        loss_train = sess.run(loss, feed_dict={input_ph: images, label_ph: labels})\n",
    "        train_losses.append(loss_train)\n",
    "        \n",
    "    sess.run(update_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    step += 1\n",
    "\n",
    "_end = time.time()\n",
    "print('Train Done! Cost Time: {:.2f}s'.format(_end - _start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(0, 5, len(train_losses), endpoint=True)\n",
    "plt.semilogy(x_axis, train_losses, label='adagrad')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.train.RMSPropOptimizer\n",
    "`tensorflow`中也集成了`RMSProp`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.RMSPropOptimizer(0.01, 0.9).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_losses1 = []\n",
    "\n",
    "epoch = 0\n",
    "samples_passed = 0\n",
    "epoch_done = False\n",
    "step = 0\n",
    "\n",
    "_start = time.time()\n",
    "while (epoch < 5):\n",
    "    if samples_passed + batch_size >= mnist.train.num_examples:\n",
    "        this_batch = mnist.train.num_examples - samples_passed\n",
    "        samples_passed = 0\n",
    "        epoch += 1\n",
    "        epoch_done = True\n",
    "    else:\n",
    "        samples_passed += batch_size\n",
    "        this_batch = batch_size\n",
    "        \n",
    "    # 获取 batch_size个训练样本\n",
    "    images, labels = train_set.next_batch(this_batch)\n",
    "    if epoch_done:\n",
    "        # 计算所有训练样本的损失值\n",
    "        train_loss = []\n",
    "        for _ in range(train_set.num_examples // 100):\n",
    "            image, label = train_set.next_batch(100)\n",
    "            loss_train = sess.run(loss, feed_dict={input_ph: image, label_ph: label})\n",
    "            train_loss.append(loss_train)\n",
    "\n",
    "        print('Epoch {} Train loss: {:.6f}'.format(epoch, np.array(train_loss).mean()))\n",
    "        epoch_done = False\n",
    "        \n",
    "    # 每30步记录一次训练误差\n",
    "    if step % 30 == 0:\n",
    "        loss_train = sess.run(loss, feed_dict={input_ph: images, label_ph: labels})\n",
    "        train_losses1.append(loss_train)\n",
    "        \n",
    "    sess.run(train_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    step += 1\n",
    "\n",
    "_end = time.time()\n",
    "print('Train Done! Cost Time: {:.2f}s'.format(_end - _start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
