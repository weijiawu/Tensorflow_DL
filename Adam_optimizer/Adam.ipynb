{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam\n",
    "Adam 是一个结合了动量法和 RMSProp 的优化算法，其结合了两者的优点。\n",
    "\n",
    "## Adam 算法\n",
    "Adam 算法会使用一个动量变量 v 和一个 RMSProp 中的梯度元素平方的移动指数加权平均 s，首先将他们全部初始化为 0，然后在每次迭代中，计算他们的移动加权平均进行更新\n",
    "\n",
    "$$\n",
    "v = \\beta_1 v + (1 - \\beta_1) g \\\\\n",
    "s = \\beta_2 s + (1 - \\beta_2) g^2\n",
    "$$\n",
    "\n",
    "在 adam 算法里，为了减轻 v 和 s 被初始化为 0 的初期对计算指数加权移动平均的影响，每次 v 和 s 都做下面的修正\n",
    "\n",
    "$$\n",
    "\\hat{v} = \\frac{v}{1 - \\beta_1^t} \\\\\n",
    "\\hat{s} = \\frac{s}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "这里 t 是迭代次数，可以看到，当 $0 \\leq \\beta_1, \\beta_2 \\leq 1$ 的时候，迭代到后期 t 比较大，那么 $\\beta_1^t$ 和 $\\beta_2^t$ 就几乎为 0，就不会对 v 和 s 有任何影响了，算法作者建议$\\beta_1 = 0.9$, $\\beta_2 = 0.999$。\n",
    "\n",
    "最后使用修正之后的 $\\hat{v}$ 和 $\\hat{s}$ 进行学习率的重新计算\n",
    "\n",
    "$$\n",
    "g' = \\frac{\\eta \\hat{v}}{\\sqrt{\\hat{s} + \\epsilon}}\n",
    "$$\n",
    "\n",
    "这里 $\\eta$ 是学习率，$epsilon$ 仍然是为了数值稳定性而添加的常数，最后参数更新有\n",
    "\n",
    "$$\n",
    "\\theta_i = \\theta_{i-1} - g'\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们来实现以下 adam 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.layers import hidden_layer, DNN\n",
    "\n",
    "tf.set_random_seed(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据导入\n",
    "mnist = input_data.read_data_sets('../MNIST_data', one_hot=True)\n",
    "\n",
    "train_set = mnist.train\n",
    "test_set = mnist.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "input_ph = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n",
    "label_ph = tf.placeholder(shape=(None, 10), dtype=tf.int64)\n",
    "\n",
    "dnn = DNN(input_ph, [200], weights_collection='params', biases_collection='params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建`loss`和`acc`\n",
    "loss = tf.losses.softmax_cross_entropy(logits=dnn, onehot_labels=label_ph)\n",
    "\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(dnn, axis=-1), tf.argmax(label_ph, axis=-1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取梯度\n",
    "params = tf.get_collection('params')\n",
    "\n",
    "gradients = tf.gradients(loss, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义`Adam`更新算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_update(params, gradients, vs, sqrs, lr, t, beta1=0.9, beta2=0.999, name='adam_update'):\n",
    "    eps = 1e-8\n",
    "    \n",
    "    update_ops = []\n",
    "    for param, gradient, v, sqr in zip(params, gradients, vs, sqrs):\n",
    "        v_update = v.assign(beta1 * v + (1 - beta1) * gradient)\n",
    "        sqr_update = sqr.assign(beta2 * sqr + (1 - beta2) * tf.square(gradient))\n",
    "        with tf.control_dependencies([v_update, sqr_update]):\n",
    "            v_hat = v / (1 - beta1 ** t)\n",
    "            s_hat = sqr / (1 - beta2 ** t)\n",
    "            update_ops.append(param.assign_sub(lr * v_hat / tf.sqrt(s_hat + eps)))\n",
    "            \n",
    "    update_op = tf.group(*update_ops, name=name)\n",
    "    return update_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义辅助变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sqrs'):\n",
    "    for i, param in enumerate(params):\n",
    "        v = tf.get_variable(param.op.name, shape=param.get_shape(), initializer=tf.zeros_initializer(), dtype=tf.float32)\n",
    "        tf.add_to_collection('sqrs', v)\n",
    "\n",
    "with tf.variable_scope('vs'):\n",
    "    for i, param in enumerate(params):\n",
    "        v = tf.get_variable(param.op.name, shape=param.get_shape(), initializer=tf.zeros_initializer(), dtype=tf.float32)\n",
    "        tf.add_to_collection('vs', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrs = tf.get_collection('sqrs')\n",
    "vs = tf.get_collection('vs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用adam定义更新`op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "\n",
    "update_op = adam_update(params, gradients, vs, sqrs, 1e-3, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "epoch = 0\n",
    "samples_passed = 0\n",
    "epoch_done = False\n",
    "step = 0\n",
    "\n",
    "_start = time.time()\n",
    "while (epoch < 5):\n",
    "    if samples_passed + batch_size >= mnist.train.num_examples:\n",
    "        this_batch = mnist.train.num_examples - samples_passed\n",
    "        samples_passed = 0\n",
    "        epoch += 1\n",
    "        epoch_done = True\n",
    "    else:\n",
    "        samples_passed += batch_size\n",
    "        this_batch = batch_size\n",
    "        \n",
    "    # 获取 batch_size个训练样本\n",
    "    images, labels = train_set.next_batch(this_batch)\n",
    "    if epoch_done:\n",
    "        # 计算所有训练样本的损失值\n",
    "        train_loss = []\n",
    "        for _ in range(train_set.num_examples // 100):\n",
    "            image, label = train_set.next_batch(100)\n",
    "            loss_train = sess.run(loss, feed_dict={input_ph: image, label_ph: label})\n",
    "            train_loss.append(loss_train)\n",
    "\n",
    "        print('Epoch {} Train loss: {:.6f}'.format(epoch, np.array(train_loss).mean()))\n",
    "        epoch_done = False\n",
    "        \n",
    "    # 每30步记录一次训练误差\n",
    "    if step % 30 == 0:\n",
    "        loss_train = sess.run(loss, feed_dict={input_ph: images, label_ph: labels})\n",
    "        train_losses.append(loss_train)\n",
    "        \n",
    "    sess.run(update_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    step += 1\n",
    "\n",
    "_end = time.time()\n",
    "print('Train Done! Cost Time: {:.2f}s'.format(_end - _start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(0, 5, len(train_losses), endpoint=True)\n",
    "plt.semilogy(x_axis, train_losses, label='adam')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.train.AdadeltaOptimizer\n",
    "`tensorflow`中也集成了`Adadelta`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_losses1 = []\n",
    "\n",
    "epoch = 0\n",
    "samples_passed = 0\n",
    "epoch_done = False\n",
    "step = 0\n",
    "\n",
    "_start = time.time()\n",
    "while (epoch < 5):\n",
    "    if samples_passed + batch_size >= mnist.train.num_examples:\n",
    "        this_batch = mnist.train.num_examples - samples_passed\n",
    "        samples_passed = 0\n",
    "        epoch += 1\n",
    "        epoch_done = True\n",
    "    else:\n",
    "        samples_passed += batch_size\n",
    "        this_batch = batch_size\n",
    "        \n",
    "    # 获取 batch_size个训练样本\n",
    "    images, labels = train_set.next_batch(this_batch)\n",
    "    if epoch_done:\n",
    "        # 计算所有训练样本的损失值\n",
    "        train_loss = []\n",
    "        for _ in range(train_set.num_examples // 100):\n",
    "            image, label = train_set.next_batch(100)\n",
    "            loss_train = sess.run(loss, feed_dict={input_ph: image, label_ph: label})\n",
    "            train_loss.append(loss_train)\n",
    "\n",
    "        print('Epoch {} Train loss: {:.6f}'.format(epoch, np.array(train_loss).mean()))\n",
    "        epoch_done = False\n",
    "        \n",
    "    # 每30步记录一次训练误差\n",
    "    if step % 30 == 0:\n",
    "        loss_train = sess.run(loss, feed_dict={input_ph: images, label_ph: labels})\n",
    "        train_losses1.append(loss_train)\n",
    "        \n",
    "    sess.run(train_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    step += 1\n",
    "\n",
    "_end = time.time()\n",
    "print('Train Done! Cost Time: {:.2f}s'.format(_end - _start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
