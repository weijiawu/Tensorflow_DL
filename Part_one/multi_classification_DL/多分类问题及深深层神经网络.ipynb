{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST手写体识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MNIST`手写体识别是图像识别中最经典的问题, 希望能够识别出人类手写的数字. 数据是65000张灰度图和对应的数字. 我们用之前的深度神经网络来尝试解决这个问题."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Tensorflow 已经把 mnist 数据集集成在 examples 里面了\n",
    "# 在这里 import 数据输入的部分\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "\n",
    "tf.set_random_seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们导入`mnist`数据集\n",
    "\n",
    "**注意** 下载数据需要一定时间, 如果下面这行代码下载出现问题, 可以从网上下载 MNIST 数据集, 然后一起放到`MNIST_data`这个文件夹中, 文件夹的结构应该是下面这样:\n",
    "```\n",
    "    MNIST_data\n",
    "        train-images-idx3-ubyte.gz\n",
    "        train-labels-idx1-ubyte.gz\n",
    "        t10k-images-idx3-ubyte.gz\n",
    "        t10k-labels-idx1-ubyte.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到`read_data_sets`的一个参数是**`one_hot`**([wiki](https://en.wikipedia.org/wiki/One-hot)). \n",
    "\n",
    "它是识别任务中非常重要的一个概念, 将一个数值`n`映射到一个向量, 这个向量的第`n`个元素是`1`, 其他元素都是`0`.\n",
    "\n",
    "这个数据集分成了两个部分: 训练和测试. 分开来是为了观察模型在完全没见过的数据上的表现, 从而体现泛化能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mnist.train\n",
    "test_set = mnist.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们具体看看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(ncols=6, nrows=2)\n",
    "plt.tight_layout(w_pad=-2.0, h_pad=-8.0)\n",
    "\n",
    "# 调用next_batch方法来一次性获取12个样本,\n",
    "# 这里有一个`shuffle`参数, 表达是否打乱样本间的顺序\n",
    "images, labels = train_set.next_batch(12, shuffle=False)\n",
    "\n",
    "for ind, (image, label) in enumerate(zip(images, labels)):\n",
    "    # image 是一个 784 维的向量, 是图片进行拉伸产生的, 这里我们给它 reshape 回去\n",
    "    image = image.reshape((28, 28))\n",
    "    \n",
    "    # label 是一个 10 维的向量, 哪个下标处的值为1 说明是数字几\n",
    "    label = label.argmax()\n",
    "\n",
    "    row = ind // 6\n",
    "    col = ind % 6\n",
    "    axes[row][col].imshow(image, cmap='gray') # 灰度图\n",
    "    axes[row][col].axis('off')\n",
    "    axes[row][col].set_title('%d' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来定义深度网络结构."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer(layer_input, output_depth, scope='hidden_layer', reuse=None):\n",
    "    input_depth = layer_input.get_shape()[-1]\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 注意这里的初始化方法是truncated_normal\n",
    "        w = tf.get_variable(initializer=tf.truncated_normal_initializer(stddev=0.1), shape=(input_depth, output_depth), name='weights')\n",
    "        # 注意这里用 0.1 对偏置进行初始化\n",
    "        b = tf.get_variable(initializer=tf.constant_initializer(0.1), shape=(output_depth), name='bias')\n",
    "        net = tf.matmul(layer_input, w) + b\n",
    "        \n",
    "        return net\n",
    "\n",
    "def DNN(x, output_depths, scope='DNN', reuse=None):\n",
    "    net = x\n",
    "    for i, output_depth in enumerate(output_depths):\n",
    "        net = hidden_layer(net, output_depth, scope='layer%d' % i, reuse=reuse)\n",
    "        # 注意这里的激活函数\n",
    "        net = tf.nn.relu(net)\n",
    "    # 数字分为0, 1, ..., 9 所以这是10分类问题\n",
    "    # 对应于 one_hot 的标签, 所以这里输出一个 10维 的向量\n",
    "    net = hidden_layer(net, 10, scope='classification', reuse=reuse)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义占位符\n",
    "input_ph = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n",
    "label_ph = tf.placeholder(shape=(None, 10), dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造一个4层的神经网络, 它的隐藏节点数分别为: 400, 200, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = DNN(input_ph, [400, 200, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是一个分类问题, 因此我们采用交叉熵来计算损失函数\n",
    "loss = tf.losses.softmax_cross_entropy(logits=dnn, onehot_labels=label_ph)\n",
    "\n",
    "# 下面定义的是正确率, 注意理解它为什么是这么定义的\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(dnn, axis=-1), tf.argmax(label_ph, axis=-1)), dtype=tf.float32))\n",
    "\n",
    "lr = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 我们训练20000次\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(20000):\n",
    "    # 获取 batch_size个训练样本\n",
    "    images, labels = train_set.next_batch(batch_size)\n",
    "    sess.run(train_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    if e % 1000 == 999:\n",
    "        # 获取 batch_size 个测试样本\n",
    "        test_imgs, test_labels = test_set.next_batch(batch_size)\n",
    "        # 计算在当前样本上的训练以及测试样本的损失值和正确率\n",
    "        loss_train, acc_train = sess.run([loss, acc], feed_dict={input_ph: images, label_ph: labels})\n",
    "        loss_test, acc_test = sess.run([loss, acc], feed_dict={input_ph: test_imgs, label_ph: test_labels})\n",
    "        print('STEP {}: train_loss: {:.6f} train_acc: {:.6f} test_loss: {:.6f} test_acc: {:.6f}'.format(e + 1, loss_train, acc_train, loss_test, acc_test))\n",
    "\n",
    "print('Train Done!')\n",
    "print('-'*30)\n",
    "\n",
    "# 计算所有训练样本的损失值以及正确率\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "for _ in range(train_set.num_examples // 100):\n",
    "    image, label = train_set.next_batch(100)\n",
    "    loss_train, acc_train = sess.run([loss, acc], feed_dict={input_ph: image, label_ph: label})\n",
    "    train_loss.append(loss_train)\n",
    "    train_acc.append(acc_train)\n",
    "\n",
    "print('Train loss: {:.6f}'.format(np.array(train_loss).mean()))\n",
    "print('Train accuracy: {:.6f}'.format(np.array(train_acc).mean()))\n",
    "\n",
    "# 计算所有测试样本的损失值以及正确率\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "for _ in range(test_set.num_examples // 100):\n",
    "    image, label = test_set.next_batch(100)\n",
    "    loss_test, acc_test = sess.run([loss, acc], feed_dict={input_ph: image, label_ph: label})\n",
    "    test_loss.append(loss_test)\n",
    "    test_acc.append(acc_test)\n",
    "\n",
    "print('Test loss: {:.6f}'.format(np.array(test_loss).mean()))\n",
    "print('Test accuracy: {:.6f}'.format(np.array(test_acc).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到, 最后在训练集上我们达到了大约0.98的正确率, 在测试集上我们达到了大约0.97的正确率, 已经是一个相当不错的成绩了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard & `tf.summary`\n",
    "\n",
    "[前面](https://github.com/SherlockLiao/ai-class-intro/blob/master/tensorflow/course_0/tensorflow-basic.ipynb)已经给大家分享过如何使用`Tensorboard`将我们构建的计算图显示出来, 这里我们还要介绍它和`tf.summary`结合起来体现的更加强大的功能: 可视化训练. \n",
    "\n",
    "首先介绍一下`tf.summary`, 它能够收集训练过程中的各种`tensor`的信息并把它保存起来以供`Tensorboard`读取并展示. 按照下面的方法来使用它:\n",
    "\n",
    "### 构造`summary`\n",
    "- - -\n",
    "- 如果你想收集表示一个标量或者一个数的`tensor`的信息, 比如上面的`loss`\n",
    "```python\n",
    "loss_sum = tf.summary.scalar('loss', loss)\n",
    "```\n",
    "上面的语句就会告诉`Tensorflow`, 在运行过程中, 我要让`Tensorboard`显示误差的变化了\n",
    "- - -\n",
    "- 如果你想收集一个`tensor`的分布情况, 这个`tensor`可以是任意形状, 比如我们定义了一个`(784, 400)`的权重`w`\n",
    "```python\n",
    "w_hist = tf.summary.histogram('w_hist', w)\n",
    "```\n",
    "- - -\n",
    "- 如果你想收集一个4维的1-通道(灰度图), 3-通道(RGB), 4-通道(RGBA)的`tensor`的变化, 比如我们输出了一个`(1, 8, 8, 1)`的灰度图`image`\n",
    "```python\n",
    "image_sum = tf.summary.image('image', image)\n",
    "```\n",
    "- - -\n",
    "- 如果你想收集一个3维(batch, frame, channel), 2维(batch, frame)的变化, 比如我们输出了一个`(10, 50, 3)`的`tensor`:`audio`\n",
    "```python\n",
    "audio_sum = tf.summary.audio('audio', audio)\n",
    "```\n",
    "- - -\n",
    "在这次课程中, 我们暂时先使用`scalar`和`histogram`的`summary`, `image`和`audio`的`summary`将在之后的课程中介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重置计算图\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 重新定义占位符\n",
    "input_ph = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n",
    "label_ph = tf.placeholder(shape=(None, 10), dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在, 我们需要重新构建前向神经网络, 为了简化代码, 我们在构造一个隐藏层以及它的参数的函数内部构造`tf.summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造权重, 用`truncated_normal`初始化\n",
    "def weight_variable(shape):\n",
    "    init = tf.truncated_normal(shape=shape, stddev=0.1)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "# 构造偏置, 用`0.1`初始化\n",
    "def bias_variable(shape):\n",
    "    init = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造添加`variable`的`summary`的函数\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        # 计算平均值\n",
    "        mean = tf.reduce_mean(var)\n",
    "        # 将平均值添加到`summary`中, 这是一个数值, 所以我们用`tf.summary.scalar`\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        \n",
    "        # 计算标准差\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        # 将标准差添加到`summary`中\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        \n",
    "        # 添加最大值,最小值`summary`\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        \n",
    "        # 添加这个变量分布情况的`summary`, 我们希望观察它的分布, 所以用`tf.summary.histogram`\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造一个隐藏层\n",
    "def hidden_layer(x, output_dim, scope='hidden_layer', act = tf.nn.relu, reuse=None):\n",
    "    # 获取输入的`depth`\n",
    "    input_dim = x.get_shape().as_list()[-1]\n",
    "    \n",
    "    with tf.name_scope(scope):\n",
    "        with tf.name_scope('weight'):\n",
    "            # 构造`weight`\n",
    "            weight = weight_variable([input_dim, output_dim])\n",
    "            # 添加`weight`的`summary`\n",
    "            variable_summaries(weight)\n",
    "            \n",
    "        with tf.name_scope('bias'):\n",
    "            # 构造`bias`\n",
    "            bias = bias_variable([output_dim])\n",
    "            # 添加`bias`的`summary`\n",
    "            variable_summaries(bias)\n",
    "            \n",
    "        with tf.name_scope('linear'):\n",
    "            # 计算`xw+b`\n",
    "            preact = tf.matmul(x, weight) + bias\n",
    "            # 添加激活层之前输出的分布情况到`summary`\n",
    "            tf.summary.histogram('pre_activation', preact)\n",
    "            \n",
    "        # 经过激活层`act`\n",
    "        output = act(preact)\n",
    "        # 添加激活后输出的分布情况到`summary`\n",
    "        tf.summary.histogram('output', output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造深度神经网络\n",
    "def DNN(x, output_depths, scope='DNN_with_sums', reuse=None):\n",
    "    with tf.name_scope(scope):\n",
    "        net = x\n",
    "        for i, output_depth in enumerate(output_depths):\n",
    "            net = hidden_layer(net, output_depth, scope='hidden%d' % (i + 1), reuse=reuse)\n",
    "        # 最后有一个分类层\n",
    "        net = hidden_layer(net, 10, scope='classification', act=tf.identity, reuse=reuse)\n",
    "        return net\n",
    "\n",
    "dnn_with_sums = DNN(input_ph, [400, 200, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新定义`loss`, `acc`, `train_op`\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    loss = tf.losses.softmax_cross_entropy(logits=dnn_with_sums, onehot_labels=label_ph)\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(dnn_with_sums, axis=-1), tf.argmax(label_ph, axis=-1)), dtype=tf.float32))\n",
    "    tf.summary.scalar('accuracy', acc)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    lr = 0.01\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (可选)融合`summary`\n",
    "- - -\n",
    "- 我们可以把前面定义的所有`summary`都融合成一个`summary`\n",
    "```python\n",
    "merged = tf.summary.merge_all()\n",
    "```\n",
    "- - -\n",
    "- 也可以只是融合某些`summary`\n",
    "```python\n",
    "merged = tf.summary.merge([loss_sum, image_sum])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出`summary`\n",
    "---\n",
    "`summary`是需要导出到外部文件的\n",
    "- 首先定义一个文件读写器\n",
    "```python\n",
    "summary_writer = tf.summary.FileWriter('summaries', sess.graph)\n",
    "```\n",
    "- - -\n",
    "- 然后在训练的过程中, 在你希望的时候运行一次`merged`或者是你之前自己定义的某个通过`summary`定义的`op`\n",
    "```python\n",
    "summaries = sess.run(merged, feed_dict={...})\n",
    "```\n",
    "- - -\n",
    "- 然后将这个`summaries`写入到`summari_writer`内\n",
    "```python\n",
    "summary_writer.add_summary(summaries, step)\n",
    "```\n",
    "注意`step`表示你当前训练的步数, 当然你也可以设定为其他你想要用的数值\n",
    "\n",
    "- - -\n",
    "- 最后关闭文件读写器\n",
    "```python\n",
    "summary_writer.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('test_summary/train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter('test_summary/test', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(20000):\n",
    "    images, labels = train_set.next_batch(batch_size)\n",
    "    sess.run(train_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    if e % 1000 == 999:\n",
    "        test_imgs, test_labels = test_set.next_batch(batch_size)\n",
    "        # 获取`train`数据的`summaries`以及`loss`, `acc`信息\n",
    "        sum_train, loss_train, acc_train = sess.run([merged, loss, acc], feed_dict={input_ph: images, label_ph: labels})\n",
    "        # 将`train`的`summaries`写入到`train_writer`中\n",
    "        train_writer.add_summary(sum_train, e)\n",
    "        # 获取`test`数据的`summaries`以及`loss`, `acc`信息\n",
    "        sum_test, loss_test, acc_test = sess.run([merged, loss, acc], feed_dict={input_ph: test_imgs, label_ph: test_labels})\n",
    "        # 将`test`的`summaries`写入到`test_writer`中\n",
    "        test_writer.add_summary(sum_test, e)\n",
    "        print('STEP {}: train_loss: {:.6f} train_acc: {:.6f} test_loss: {:.6f} test_acc: {:.6f}'.format(e + 1, loss_train, acc_train, loss_test, acc_test))\n",
    "\n",
    "# 关闭读写器\n",
    "train_writer.close()\n",
    "test_writer.close()\n",
    "\n",
    "print('Train Done!')\n",
    "print('-'*30)\n",
    "\n",
    "# 计算所有训练样本的损失值以及正确率\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "for _ in range(train_set.num_examples // 100):\n",
    "    image, label = train_set.next_batch(100)\n",
    "    loss_train, acc_train = sess.run([loss, acc], feed_dict={input_ph: image, label_ph: label})\n",
    "    train_loss.append(loss_train)\n",
    "    train_acc.append(acc_train)\n",
    "\n",
    "print('Train loss: {:.6f}'.format(np.array(train_loss).mean()))\n",
    "print('Train accuracy: {:.6f}'.format(np.array(train_acc).mean()))\n",
    "\n",
    "# 计算所有测试样本的损失值以及正确率\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "for _ in range(test_set.num_examples // 100):\n",
    "    image, label = test_set.next_batch(100)\n",
    "    loss_test, acc_test = sess.run([loss, acc], feed_dict={input_ph: image, label_ph: label})\n",
    "    test_loss.append(loss_test)\n",
    "    test_acc.append(acc_test)\n",
    "\n",
    "print('Test loss: {:.6f}'.format(np.array(test_loss).mean()))\n",
    "print('Test accuracy: {:.6f}'.format(np.array(test_acc).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开`Tensorboard`\n",
    "在之前对计算图可视化的时候, 我们用`tensorboard --logdir=.`命令打开过`Tensorboard`显示当前目录下``. 但`Tensorboard`支持打开多个目录下的`.events`文件, 方便我们对比不同模型或者训练和测试之间的差别\n",
    "\n",
    "\n",
    "在`test_summary`目录中输入以下命令\n",
    "```\n",
    "$ tensorboard --logdir=train:train/,test:test/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们就可以看到类似于下面的界面了\n",
    "- **计算图**\n",
    "<img src=\"https://image.ibb.co/fGfMSc/graph.png\" width=\"500\">\n",
    "- - -\n",
    "- **`loss`和`accuracy`**\n",
    "<figure class=\"half\">\n",
    "    <img src=\"https://image.ibb.co/n1bBSc/loss.png\" align=\"left\">\n",
    "    <img src=\"https://image.ibb.co/dHg0Lx/accuracy.png\" align='right'>\n",
    "</figure>\n",
    "- - -\n",
    "- **所有的标量统计**\n",
    "<img src=\"https://image.ibb.co/kVP9DH/scalars.png\" width=\"800\">\n",
    "- - -\n",
    "- **所有的直方图统计**\n",
    "<img src=\"https://image.ibb.co/bx60Lx/histogram.png\" width=\"800\">\n",
    "- - - \n",
    "- **所有的分布统计**\n",
    "<img src=\"https://image.ibb.co/irS70x/distribution.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tensorboard`有各种各样的有意思的地方, 大家可以多探索探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结语\n",
    "这次大家学习到了如何对`MNIST`手写体识别问题构建深度神经网络模型, 以及如何使用`Tensorboard`对训练过程进行可视化. 在接下来的课程中, 我们将要进入到深度学习的卷积神经网络部分"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
