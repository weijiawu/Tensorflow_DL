{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机梯度下降法\n",
    "前面我们介绍了梯度下降法的数学原理，下面我们通过例子来说明一下随机梯度下降法，我们分别从 0 自己实现，以及使用 pytorch 中自带的优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Tensorflow 已经把 mnist 数据集集成在 examples 里面了\n",
    "# 在这里 import 数据输入的部分\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "\n",
    "tf.set_random_seed(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets('../MNIST_data', one_hot=True)\n",
    "\n",
    "train_set = mnist.train\n",
    "test_set = mnist.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型\n",
    "我们把之前写过的函数封装进`utils.layers`中并调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.layers import hidden_layer, DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义占位符\n",
    "input_ph = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n",
    "label_ph = tf.placeholder(shape=(None, 10), dtype=tf.int64)\n",
    "\n",
    "dnn = DNN(input_ph, [200], weights_collection='params', biases_collection='params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建`loss`和`acc`\n",
    "loss = tf.losses.softmax_cross_entropy(logits=dnn, onehot_labels=label_ph)\n",
    "\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(dnn, axis=-1), tf.argmax(label_ph, axis=-1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里, 我们利用到`tensorflow`中的函数**`tf.add_to_collection`**,**`tf.get_collection`**\n",
    "\n",
    "- `tf.add_to_collection(name, var)`会将`var`加入到名为`name`的`collection`的收集列表里面\n",
    "- `tf.get_collection(name)`则会将名为`name`的收集列表中所有的变量取出\n",
    "\n",
    "那么我们在定义变量的过程中就可以把他们加到名为`params`的列表中, 然后通过`get_collection`一次性取出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tf.get_collection('params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过`tf.gradients`计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients_for_params = tf.gradients(loss, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义`SGD`下降算法\n",
    "随机梯度下降法非常简单，公式就是\n",
    "$$\n",
    "\\theta_{i+1} = \\theta_i - \\eta \\nabla L(\\theta)\n",
    "$$\n",
    "非常简单，我们可以从 0 开始自己实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_update(params, gradients, lr, name='update_op'):\n",
    "    update_ops = []\n",
    "    for param, gradient in zip(params, gradients):\n",
    "        # 对每个参数,减去学习率乘上梯度\n",
    "        update_ops.append(param.assign_sub(lr * gradient))\n",
    "        \n",
    "    # 用`tf.group`将所有的更新`op`合并成一个`op`\n",
    "    update_op = tf.group(*update_ops, name=name)\n",
    "    return update_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置学习率为0.01\n",
    "update_op = sgd_update(params, gradients_for_params, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练\n",
    "- 训练5个epoch\n",
    "- 设置`batch_size=1`,也就是每过一个样本便更新一次参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们训练5个epoch, 设置`batch_size=1`\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "epoch = 0\n",
    "samples_passed = 0\n",
    "epoch_done = False\n",
    "step = 0\n",
    "\n",
    "_start = time.time()\n",
    "while (epoch < 5):\n",
    "    if samples_passed + batch_size >= mnist.train.num_examples:\n",
    "        this_batch = mnist.train.num_examples - samples_passed\n",
    "        samples_passed = 0\n",
    "        epoch += 1\n",
    "        epoch_done = True\n",
    "    else:\n",
    "        samples_passed += batch_size\n",
    "        this_batch = batch_size\n",
    "        \n",
    "    # 获取 batch_size个训练样本\n",
    "    images, labels = train_set.next_batch(this_batch)\n",
    "    # 每30步记录一次误差\n",
    "    if epoch_done:\n",
    "        # 计算所有训练样本的损失值\n",
    "        train_loss = []\n",
    "        for _ in range(train_set.num_examples // 100):\n",
    "            image, label = train_set.next_batch(100)\n",
    "            loss_train = sess.run(loss, feed_dict={input_ph: image, label_ph: label})\n",
    "            train_loss.append(loss_train)\n",
    "\n",
    "        print('Epoch {} Train loss: {:.6f}'.format(epoch, np.array(train_loss).mean()))\n",
    "        epoch_done = False\n",
    "        \n",
    "    if step % 30 == 0:\n",
    "        loss_train = sess.run(loss, feed_dict={input_ph: images, label_ph: labels})\n",
    "        train_losses.append(loss_train)\n",
    "        \n",
    "    sess.run(update_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    step += 1\n",
    "\n",
    "_end = time.time()\n",
    "print('Train Done! Cost Time: {:.2f}s'.format(_end - _start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(0, 5, len(train_losses), endpoint=True)\n",
    "plt.semilogy(x_axis, train_losses, label='batch_size=1')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到, 训练误差震荡非常厉害, 非常不稳定, 而且训练时间非常长, 接下来我们试着将`batch_size`改为64\n",
    "\n",
    "### 增大`batch_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们设置`batch_size=64`\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "epoch = 0\n",
    "samples_passed = 0\n",
    "epoch_done = False\n",
    "step = 0\n",
    "\n",
    "_start = time.time()\n",
    "while (epoch < 5):\n",
    "    if samples_passed + batch_size >= mnist.train.num_examples:\n",
    "        this_batch = mnist.train.num_examples - samples_passed\n",
    "        samples_passed = 0\n",
    "        epoch += 1\n",
    "        epoch_done = True\n",
    "    else:\n",
    "        samples_passed += batch_size\n",
    "        this_batch = batch_size\n",
    "        \n",
    "    # 获取 batch_size个训练样本\n",
    "    images, labels = train_set.next_batch(this_batch)\n",
    "    if epoch_done:\n",
    "        # 计算所有训练样本的损失值\n",
    "        train_loss = []\n",
    "        for _ in range(train_set.num_examples // 100):\n",
    "            image, label = train_set.next_batch(100)\n",
    "            loss_train = sess.run(loss, feed_dict={input_ph: image, label_ph: label})\n",
    "            train_loss.append(loss_train)\n",
    "\n",
    "        print('Epoch {} Train loss: {:.6f}'.format(epoch, np.array(train_loss).mean()))\n",
    "        epoch_done = False\n",
    "        \n",
    "    # 每30步记录一次训练误差\n",
    "    if step % 30 == 0:\n",
    "        loss_train = sess.run(loss, feed_dict={input_ph: images, label_ph: labels})\n",
    "        train_losses.append(loss_train)\n",
    "        \n",
    "    sess.run(update_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    step += 1\n",
    "\n",
    "_end = time.time()\n",
    "print('Train Done! Cost Time: {:.2f}s'.format(_end - _start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(0, 5, len(train_losses), endpoint=True)\n",
    "plt.semilogy(x_axis, train_losses, label='batch_size=64')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的结果可以看到 loss 没有 batch 等于 1 震荡那么距离，同时也可以降到一定的程度了，时间上也比之前快了非常多，因为按照 batch 的数据量计算上更快，同时梯度对比于 batch size = 1 的情况也跟接近真实的梯度，所以 batch size 的值越大，梯度也就越稳定，而 batch size 越小，梯度具有越高的随机性，这里 batch size 为 64，可以看到 loss 仍然存在震荡，但这并没有关系，如果 batch size 太大，对于内存的需求就更高，同时也不利于网络跳出局部极小点，所以现在普遍使用基于 batch 的随机梯度下降法，而 batch 的多少基于实际情况进行考虑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 大学习率\n",
    "现在我们试着改变学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_op2 = sgd_update(params, gradients_for_params, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习率是2.0\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "epoch = 0\n",
    "samples_passed = 0\n",
    "epoch_done = False\n",
    "step = 0\n",
    "\n",
    "_start = time.time()\n",
    "while (epoch < 5):\n",
    "    if samples_passed + batch_size >= mnist.train.num_examples:\n",
    "        this_batch = mnist.train.num_examples - samples_passed\n",
    "        samples_passed = 0\n",
    "        epoch += 1\n",
    "        epoch_done = True\n",
    "    else:\n",
    "        samples_passed += batch_size\n",
    "        this_batch = batch_size\n",
    "        \n",
    "    # 获取 batch_size个训练样本\n",
    "    images, labels = train_set.next_batch(this_batch)\n",
    "    if epoch_done:\n",
    "        # 计算所有训练样本的损失值\n",
    "        train_loss = []\n",
    "        for _ in range(train_set.num_examples // 100):\n",
    "            image, label = train_set.next_batch(100)\n",
    "            loss_train = sess.run(loss, feed_dict={input_ph: image, label_ph: label})\n",
    "            train_loss.append(loss_train)\n",
    "\n",
    "        print('Epoch {} Train loss: {:.6f}'.format(epoch, np.array(train_loss).mean()))\n",
    "        epoch_done = False\n",
    "        \n",
    "    # 每30步记录一次训练误差\n",
    "    if step % 30 == 0:\n",
    "        loss_train = sess.run(loss, feed_dict={input_ph: images, label_ph: labels})\n",
    "        train_losses.append(loss_train)\n",
    "    \n",
    "    # 使用1.0的学习率\n",
    "    sess.run(update_op2, feed_dict={input_ph: images, label_ph: labels})\n",
    "    step += 1\n",
    "\n",
    "_end = time.time()\n",
    "print('Train Done! Cost Time: {:.2f}s'.format(_end - _start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(0, 5, len(train_losses), endpoint=True)\n",
    "plt.semilogy(x_axis, train_losses, label='lr=2.0')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到, 训练误差一直在较高的位置震荡, 难以收敛, 因此我们一般不用大学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SGD` in `Tensorflow`\n",
    "`tensorflow`也集成了`sgd`算法, 前面我们也使用过, 那么现在回顾一下`tensorflow`的`SGD`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们设置`batch_size=64`\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "epoch = 0\n",
    "samples_passed = 0\n",
    "epoch_done = False\n",
    "step = 0\n",
    "\n",
    "_start = time.time()\n",
    "while (epoch < 5):\n",
    "    if samples_passed + batch_size >= mnist.train.num_examples:\n",
    "        this_batch = mnist.train.num_examples - samples_passed\n",
    "        samples_passed = 0\n",
    "        epoch += 1\n",
    "        epoch_done = True\n",
    "    else:\n",
    "        samples_passed += batch_size\n",
    "        this_batch = batch_size\n",
    "        \n",
    "    # 获取 batch_size个训练样本\n",
    "    images, labels = train_set.next_batch(this_batch)\n",
    "    if epoch_done:\n",
    "        # 计算所有训练样本的损失值\n",
    "        train_loss = []\n",
    "        for _ in range(train_set.num_examples // 100):\n",
    "            image, label = train_set.next_batch(100)\n",
    "            loss_train = sess.run(loss, feed_dict={input_ph: image, label_ph: label})\n",
    "            train_loss.append(loss_train)\n",
    "\n",
    "        print('Epoch {} Train loss: {:.6f}'.format(epoch, np.array(train_loss).mean()))\n",
    "        epoch_done = False\n",
    "        \n",
    "    # 每30步记录一次训练误差\n",
    "    if step % 30 == 0:\n",
    "        loss_train = sess.run(loss, feed_dict={input_ph: images, label_ph: labels})\n",
    "        train_losses.append(loss_train)\n",
    "    \n",
    "    # 使用`tensorflow`自带的`SGD`\n",
    "    sess.run(train_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    step += 1\n",
    "\n",
    "_end = time.time()\n",
    "print('Train Done! Cost Time: {:.2f}s'.format(_end - _start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(0, 5, len(train_losses), endpoint=True)\n",
    "plt.semilogy(x_axis, train_losses, label='tensorflow-sgd')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
