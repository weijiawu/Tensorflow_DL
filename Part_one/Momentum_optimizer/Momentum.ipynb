{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动量法\n",
    "使用梯度下降法，每次都会朝着目标函数下降最快的方向，这也称为最速下降法。这种更新方法看似非常快，实际上存在一些问题。\n",
    "\n",
    "## 梯度下降法的问题\n",
    "考虑一个二维输入，$[x_1, x_2]$，输出的损失函数 $L: R^2 \\rightarrow R$，下面是这个函数的等高线\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tKfTcly1fmnketw5f4j30az04lq31.jpg)\n",
    "\n",
    "可以想象成一个很扁的漏斗，这样在竖直方向上，梯度就非常大，在水平方向上，梯度就相对较小，所以我们在设置学习率的时候就不能设置太大，为了防止竖直方向上参数更新太过了，这样一个较小的学习率又导致了水平方向上参数在更新的时候太过于缓慢，所以就导致最终收敛起来非常慢。\n",
    "\n",
    "## 动量法\n",
    "动量法的提出就是为了应对这个问题，我们梯度下降法做一个修改如下\n",
    "\n",
    "$$\n",
    "v_i = \\gamma v_{i-1} + \\eta \\nabla L(\\theta)\n",
    "$$\n",
    "$$\n",
    "\\theta_i = \\theta_{i-1} - v_i\n",
    "$$\n",
    "\n",
    "其中 $v_i$ 是当前速度，$\\gamma$ 是动量参数，是一个小于 1的正数，$\\eta$ 是学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相当于每次在进行参数更新的时候，都会将之前的速度考虑进来，每个参数在各方向上的移动幅度不仅取决于当前的梯度，还取决于过去各个梯度在各个方向上是否一致，如果一个梯度一直沿着当前方向进行更新，那么每次更新的幅度就越来越大，如果一个梯度在一个方向上不断变化，那么其更新幅度就会被衰减，这样我们就可以使用一个较大的学习率，使得收敛更快，同时梯度比较大的方向就会因为动量的关系每次更新的幅度减少，如下图\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tNc79gy1fmo5l53o76j30ak04gjrh.jpg)\n",
    "\n",
    "比如我们的梯度每次都等于 g，而且方向都相同，那么动量法在该方向上使参数加速移动，有下面的公式：\n",
    "\n",
    "$$\n",
    "v_0 = 0\n",
    "$$\n",
    "$$\n",
    "v_1 = \\gamma v_0 + \\eta g = \\eta g\n",
    "$$\n",
    "$$\n",
    "v_2 = \\gamma v_1 + \\eta g = (1 + \\gamma) \\eta g\n",
    "$$\n",
    "$$\n",
    "v_3 = \\gamma v_2 + \\eta g = (1 + \\gamma + \\gamma^2) \\eta g\n",
    "$$\n",
    "$$\n",
    "\\cdots\n",
    "$$\n",
    "$$\n",
    "v_{+ \\infty} = (1 + \\gamma + \\gamma^2 + \\gamma^3 + \\cdots) \\eta g = \\frac{1}{1 - \\gamma} \\eta g\n",
    "$$\n",
    "\n",
    "如果我们把 $\\gamma$ 定为 0.9，那么更新幅度的峰值就是原本梯度乘学习率的 10 倍。\n",
    "\n",
    "本质上说，动量法就仿佛我们从高坡上推一个球，小球在向下滚动的过程中积累了动量，在途中也会变得越来越快，最后会达到一个峰值，对应于我们的算法中就是，动量项会沿着梯度指向方向相同的方向不断增大，对于梯度方向改变的方向逐渐减小，得到了更快的收敛速度以及更小的震荡。\n",
    "\n",
    "下面我们手动实现一个动量法，公式已经在上面了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.layers import hidden_layer, DNN\n",
    "\n",
    "tf.set_random_seed(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据导入\n",
    "mnist = input_data.read_data_sets('../MNIST_data', one_hot=True)\n",
    "\n",
    "train_set = mnist.train\n",
    "test_set = mnist.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "input_ph = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n",
    "label_ph = tf.placeholder(shape=(None, 10), dtype=tf.int64)\n",
    "\n",
    "dnn = DNN(input_ph, [200], weights_collection='params', biases_collection='params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建`loss`和`acc`\n",
    "loss = tf.losses.softmax_cross_entropy(logits=dnn, onehot_labels=label_ph)\n",
    "\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(dnn, axis=-1), tf.argmax(label_ph, axis=-1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取梯度\n",
    "params = tf.get_collection('params')\n",
    "\n",
    "gradients = tf.gradients(loss, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义`momentum`更新算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_update(params, gradients, vs, lr, gamma, name='momentum_update'):\n",
    "    update_ops = []\n",
    "    for param, gradient, v in zip(params, gradients, vs):\n",
    "        # 首先更新速度\n",
    "        v_update = v.assign(gamma * v + lr * gradient)\n",
    "        # 用`tf.control_dependencies`确定先更新速度后更新参数的逻辑\n",
    "        with tf.control_dependencies([v_update]):\n",
    "            update_ops.append(param.assign_sub(v))\n",
    "    \n",
    "    # 将所有参数更新算子糅合成一个算子\n",
    "    update_op = tf.group(*update_ops, name=name)\n",
    "    return update_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义`速度`变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('moments'):\n",
    "    for i, param in enumerate(params):\n",
    "        v = tf.get_variable(param.op.name, shape=param.get_shape(), initializer=tf.zeros_initializer(), dtype=tf.float32)\n",
    "        tf.add_to_collection('moments', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = tf.get_collection('moments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用momentum定义更新`op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_op = momentum_update(params, gradients, vs, 0.01, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "epoch = 0\n",
    "samples_passed = 0\n",
    "epoch_done = False\n",
    "step = 0\n",
    "\n",
    "_start = time.time()\n",
    "while (epoch < 5):\n",
    "    if samples_passed + batch_size >= mnist.train.num_examples:\n",
    "        this_batch = mnist.train.num_examples - samples_passed\n",
    "        samples_passed = 0\n",
    "        epoch += 1\n",
    "        epoch_done = True\n",
    "    else:\n",
    "        samples_passed += batch_size\n",
    "        this_batch = batch_size\n",
    "        \n",
    "    # 获取 batch_size个训练样本\n",
    "    images, labels = train_set.next_batch(this_batch)\n",
    "    if epoch_done:\n",
    "        # 计算所有训练样本的损失值\n",
    "        train_loss = []\n",
    "        for _ in range(train_set.num_examples // 100):\n",
    "            image, label = train_set.next_batch(100)\n",
    "            loss_train = sess.run(loss, feed_dict={input_ph: image, label_ph: label})\n",
    "            train_loss.append(loss_train)\n",
    "\n",
    "        print('Epoch {} Train loss: {:.6f}'.format(epoch, np.array(train_loss).mean()))\n",
    "        epoch_done = False\n",
    "        \n",
    "    # 每30步记录一次训练误差\n",
    "    if step % 30 == 0:\n",
    "        loss_train = sess.run(loss, feed_dict={input_ph: images, label_ph: labels})\n",
    "        train_losses.append(loss_train)\n",
    "        \n",
    "    sess.run(update_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    step += 1\n",
    "\n",
    "_end = time.time()\n",
    "print('Train Done! Cost Time: {:.2f}s'.format(_end - _start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(0, 5, len(train_losses), endpoint=True)\n",
    "plt.semilogy(x_axis, train_losses, label='momentum=0.9')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.train.MomentumOptimizer\n",
    "`tensorflow`中也集成了`momentum`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.MomentumOptimizer(0.01, 0.9).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_losses1 = []\n",
    "\n",
    "epoch = 0\n",
    "samples_passed = 0\n",
    "epoch_done = False\n",
    "step = 0\n",
    "\n",
    "_start = time.time()\n",
    "while (epoch < 5):\n",
    "    if samples_passed + batch_size >= mnist.train.num_examples:\n",
    "        this_batch = mnist.train.num_examples - samples_passed\n",
    "        samples_passed = 0\n",
    "        epoch += 1\n",
    "        epoch_done = True\n",
    "    else:\n",
    "        samples_passed += batch_size\n",
    "        this_batch = batch_size\n",
    "        \n",
    "    # 获取 batch_size个训练样本\n",
    "    images, labels = train_set.next_batch(this_batch)\n",
    "    if epoch_done:\n",
    "        # 计算所有训练样本的损失值\n",
    "        train_loss = []\n",
    "        for _ in range(train_set.num_examples // 100):\n",
    "            image, label = train_set.next_batch(100)\n",
    "            loss_train = sess.run(loss, feed_dict={input_ph: image, label_ph: label})\n",
    "            train_loss.append(loss_train)\n",
    "\n",
    "        print('Epoch {} Train loss: {:.6f}'.format(epoch, np.array(train_loss).mean()))\n",
    "        epoch_done = False\n",
    "        \n",
    "    # 每30步记录一次训练误差\n",
    "    if step % 30 == 0:\n",
    "        loss_train = sess.run(loss, feed_dict={input_ph: images, label_ph: labels})\n",
    "        train_losses1.append(loss_train)\n",
    "        \n",
    "    sess.run(train_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    step += 1\n",
    "\n",
    "_end = time.time()\n",
    "print('Train Done! Cost Time: {:.2f}s'.format(_end - _start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对比`SGD`\n",
    "我们再来对比一下`SGD`的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op2 = tf.train.GradientDescentOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_losses2 = []\n",
    "\n",
    "epoch = 0\n",
    "samples_passed = 0\n",
    "epoch_done = False\n",
    "step = 0\n",
    "\n",
    "_start = time.time()\n",
    "while (epoch < 5):\n",
    "    if samples_passed + batch_size >= mnist.train.num_examples:\n",
    "        this_batch = mnist.train.num_examples - samples_passed\n",
    "        samples_passed = 0\n",
    "        epoch += 1\n",
    "        epoch_done = True\n",
    "    else:\n",
    "        samples_passed += batch_size\n",
    "        this_batch = batch_size\n",
    "        \n",
    "    # 获取 batch_size个训练样本\n",
    "    images, labels = train_set.next_batch(this_batch)\n",
    "    if epoch_done:\n",
    "        # 计算所有训练样本的损失值\n",
    "        train_loss = []\n",
    "        for _ in range(train_set.num_examples // 100):\n",
    "            image, label = train_set.next_batch(100)\n",
    "            loss_train = sess.run(loss, feed_dict={input_ph: image, label_ph: label})\n",
    "            train_loss.append(loss_train)\n",
    "\n",
    "        print('Epoch {} Train loss: {:.6f}'.format(epoch, np.array(train_loss).mean()))\n",
    "        epoch_done = False\n",
    "        \n",
    "    # 每30步记录一次训练误差\n",
    "    if step % 30 == 0:\n",
    "        loss_train = sess.run(loss, feed_dict={input_ph: images, label_ph: labels})\n",
    "        train_losses2.append(loss_train)\n",
    "        \n",
    "    sess.run(train_op2, feed_dict={input_ph: images, label_ph: labels})\n",
    "    step += 1\n",
    "\n",
    "_end = time.time()\n",
    "print('Train Done! Cost Time: {:.2f}s'.format(_end - _start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(0, 5, len(train_losses), endpoint=True)\n",
    "plt.semilogy(x_axis, train_losses1, label='momentum: 0.9')\n",
    "plt.semilogy(x_axis, train_losses2, label='no momentum')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到, 动量法比`sgd`收敛更快, 可以将动量理解为一种惯性作用，所以每次更新的幅度都会比不加动量的情况更多"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
